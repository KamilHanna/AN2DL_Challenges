{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omSLbdLvhDRx"
   },
   "source": [
    "### Connect to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "AoaLQpvChLpb",
    "outputId": "ea2b737e-0fcc-4600-9708-daa0801d7079"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd ../gdrive/MyDrive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdD_8Vyswkwf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOUG_ObhGxhM",
    "outputId": "ea32ef5d-fcea-4e12-9b8a-1933cf05b022"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "import logging\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.applications.convnext import preprocess_input\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing Cyclical Learning Rate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWPlbl4pG5GI",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"\n",
    "    code taken from https://github.com/bckenstler/CLR/blob/master/clr_callback.py\n",
    "\n",
    "    This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or\n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "\n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "\n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored\n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma ** (x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n",
    "                self.clr_iterations)\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load and process the dataset removing outliers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cug9tuKXHQXo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d4fb2141-3d06-453a-e62c-5cd7867efb3c"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "plt.ion()\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)\n",
    "npz_file_path = 'public_data.npz'  \n",
    "\n",
    "# Load data from the NPZ file\n",
    "data = np.load(npz_file_path, allow_pickle=True)\n",
    "\n",
    "\n",
    "image_data = data['data']\n",
    "labels_data = data['labels']\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for lbl in labels_data:\n",
    "    if lbl == 'healthy':\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "\n",
    "for img in image_data:\n",
    "    # Normalizing images\n",
    "    dim = min(img.shape[:-1])\n",
    "    img = img[(img.shape[0]-dim)//2:(img.shape[0]+dim)//2, (img.shape[1]-dim)//2:(img.shape[1]+dim)//2, :]\n",
    "    img = tfkl.Resizing(96, 96)(img)\n",
    "    images.append(img)\n",
    "images = np.array(images)\n",
    "\n",
    "\n",
    "##SHREK REMOVAL\n",
    "first_image = images[58]\n",
    "mse_distances = [np.mean((first_image - image) ** 2) for image in images]\n",
    "\n",
    "# Combine the images and their MSE distances\n",
    "image_data = list(zip(images, mse_distances))\n",
    "\n",
    "# Sort the images based on MSE distances (in ascending order)\n",
    "image_data.sort(key=lambda x: x[1])\n",
    "\n",
    "# Select the top 100 images with the lowest MSE distances\n",
    "top_100_images = [image for image, _ in image_data[:98]]\n",
    "\n",
    "new_images = []\n",
    "new_y = []\n",
    "\n",
    "for image, label in zip(images, labels):\n",
    "    if not any(np.array_equal(image, top_image) for top_image in top_100_images):\n",
    "        new_images.append(image)\n",
    "        new_y.append(label)\n",
    "\n",
    "# Update images and labels\n",
    "images = new_images\n",
    "labels = new_y\n",
    "images = np.array(new_images)\n",
    "\n",
    "### TROLOLO REMOVAL\n",
    "first_image = images[332]\n",
    "\n",
    "mse_distances = [np.mean((first_image - image) ** 2) for image in images]\n",
    "\n",
    "# Combine the images and their MSE distances\n",
    "image_data = list(zip(images, mse_distances))\n",
    "\n",
    "# Sort the images based on MSE distances (in ascending order)\n",
    "image_data.sort(key=lambda x: x[1])\n",
    "\n",
    "# Select the top 100 images with the lowest MSE distances\n",
    "top_100_images = [image for image, _ in image_data[:98]]\n",
    "new_images = []\n",
    "new_y = []\n",
    "\n",
    "for image, label in zip(images, labels):\n",
    "    if not any(np.array_equal(image, top_image) for top_image in top_100_images):\n",
    "        new_images.append(image)\n",
    "        new_y.append(label)\n",
    "\n",
    "# Update images and labels\n",
    "images = new_images\n",
    "labels = new_y\n",
    "images = np.array(new_images)\n",
    "\n",
    "\n",
    "\n",
    "X = images\n",
    "y = labels\n",
    "y = tfk.utils.to_categorical(labels,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Splitting sets and balancing classes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGQOZ4DWHlxI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "34eac80d-4ba1-44ae-e253-e8514ec42608"
   },
   "outputs": [],
   "source": [
    "# Split train_val into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=500, stratify=np.argmax(y_train_val,axis=1))\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=seed)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train.reshape(-1, 96*96*3), y_train)\n",
    "y_resampled = tfk.utils.to_categorical(y_resampled,len(np.unique(y_resampled)))\n",
    "\n",
    "# Reshape the resampled data back to the original shape\n",
    "X_resampled = X_resampled.reshape((-1, 96, 96, 3))\n",
    "X_train = X_resampled\n",
    "y_train = y_resampled\n",
    "print(f\"X_train shape: {X_train.shape}, Y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, Y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transfer learning import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2XYLlP-HwQF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "57f5bea9-dead-40be-959e-674768b19f08"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_large_notop.h5\n",
      "785596384/785596384 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "transfer = tfk.applications.ConvNeXtLarge(\n",
    "    input_shape=(96, 96, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    pooling='avg',\n",
    ")\n",
    "transfer.trainable = True\n",
    "for i in range(90):\n",
    "  transfer.layers[i].trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model build"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JV9fdPfiIEC4"
   },
   "outputs": [],
   "source": [
    "tl_model = tf.keras.Sequential([\n",
    "    tfk.Input(shape=(96, 96, 3)),\n",
    "    Dropout(0.1),\n",
    "    transfer,\n",
    "    BatchNormalization(),\n",
    "    tfkl.Dense(2, activation='sigmoid')  # Adjust num_classes accordingly\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model compile and learning rate implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPFjGb0yIM0B",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5c38e91f-7d9d-43ff-d936-3f6b6ebc1e4b"
   },
   "outputs": [],
   "source": [
    "tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
    "tl_model.summary()\n",
    "batch_size=512\n",
    "training_samples = int(len(X)*batch_size)\n",
    "step_size = 4*training_samples // batch_size\n",
    "lr_schedule = CyclicLR(\n",
    "    mode='triangular',\n",
    "    base_lr=1e-5,\n",
    "    max_lr=1e-4,\n",
    "    step_size= step_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training and saving the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "tl_history = tl_model.fit(\n",
    "    x = preprocess_input(X_train), # We need to apply the preprocessing thought for the transferred network\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = 50,\n",
    "    validation_data = (preprocess_input(X_val), y_val), # We need to apply the preprocessing thought for the transferred network\n",
    "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=True), lr_schedule]\n",
    ").history\n",
    "save_model_dir = 'savemymodel'\n",
    "os.makedirs(save_model_dir, exist_ok=True)\n",
    "tf.keras.models.save_model(tl_model, os.path.join(save_model_dir, 'ColabSubmissionModel'))\n",
    "print(\"Model saved to:\", save_model_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(tl_history['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')\n",
    "plt.plot(tl_history['val_accuracy'], label='Transfer Learning', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The model.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "class model:\n",
    "    def __init__(self, path):\n",
    "        self.model = tf.keras.models.load_model(os.path.join(path, 'ColabSubmissionModel'))\n",
    "\n",
    "    def preprocess_image(self, img):\n",
    "        dim = min(img.shape[:-1])\n",
    "        img = img[(img.shape[0] - dim) // 2:(img.shape[0] + dim) // 2,\n",
    "              (img.shape[1] - dim) // 2:(img.shape[1] + dim) // 2, :]\n",
    "        img = tfkl.Resizing(96, 96)(img)\n",
    "        img = preprocess_input(img)\n",
    "        return img\n",
    "    def predict(self, X):\n",
    "        # Preprocess the images before making predictions\n",
    "        preprocessed_images = [self.preprocess_image(img) for img in X]\n",
    "\n",
    "        # Convert the list of images to a NumPy array\n",
    "        preprocessed_images = np.array(preprocessed_images)\n",
    "\n",
    "        # Make predictions using the model\n",
    "        predictions = self.model.predict(preprocessed_images)\n",
    "\n",
    "        # Convert predictions to class labels\n",
    "        predicted_classes = np.argmax(predictions, axis=-1)\n",
    "        out = tf.convert_to_tensor(predicted_classes)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
